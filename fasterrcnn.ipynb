{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "KWAvkeE4yOKQ"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from astropy.io import fits\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "%env CUDA_LAUNCH_BLOCKING=1\n",
    "print(os.environ['CUDA_LAUNCH_BLOCKING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qVp4A9MLyOKc",
    "outputId": "8e219bec-b653-4a65-95e1-b67e68079edf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float32 # We will be using float throughout this tutorial.\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(f\"Using device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AwfRU6uUyOKb"
   },
   "outputs": [],
   "source": [
    "# Shuffle and split dataset\n",
    "# Commenting this out for now, but we may want to reintroduce this at some point\n",
    "# shuffler = np. random. permutation(len(X))\n",
    "# X = torch.stack([X[i] for i in shuffler])\n",
    "# y = torch.tensor([y[i] for i in shuffler])\n",
    "# X_train = X[0:int(len(X) * 0.8)]\n",
    "# X_test = X[int(len(X) * 0.8):]\n",
    "# y_train = y[0:int(len(y) * 0.8)]\n",
    "# y_test = y[int(len(y) * 0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import PlanetDataset\n",
    "\n",
    "train_path = '../data/train'\n",
    "test_path = '../data/test'\n",
    "train_dataset = PlanetDataset(train_path, None, device)\n",
    "test_dataset = PlanetDataset(test_path, None, False, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "DRyHf_AoyOKY",
    "outputId": "e326e9b2-6e50-4031-d8bd-a1aed1f95bb9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing as sanity check\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# trans = transforms.Compose([transforms.ToTensor()])\n",
    "demo_img = train_dataset[2][0]\n",
    "# demo_img = trans(demo)\n",
    "# print(demo_img.shape)\n",
    "demo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)\n",
    "demo_img = torch.from_numpy(demo_array)\n",
    "# print(demo_img.shape)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(demo_img, cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2OiHkFEDyOKd"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(dataset, model):\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')   \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YOOM9aYnyOKe"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, dataset, collate_fn, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a model PyTorch Module API.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        print(f'Beginning epoch {e + 1}')\n",
    "        loss = None\n",
    "        model.train()  # put model to training mode\n",
    "        data_loader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4, collate_fn=collate_fn)\n",
    "#         counter = 0\n",
    "        for imgs, targets in data_loader:\n",
    "            \n",
    "            for i in range(len(targets)):\n",
    "                # print(len(targets[i]['boxes']) == len(targets[i]['labels']))\n",
    "#                 print('boxes ', targets[i]['boxes'].shape)\n",
    "#                 print('labels ', targets[i]['labels'].shape)\n",
    "#                 print('image_id ', targets[i]['image_id'].shape)\n",
    "#                 print('area ', targets[i]['area'].shape)\n",
    "#                 print('iscrowd ', targets[i]['iscrowd'].shape)\n",
    "                print('labels', targets[i]['labels'])\n",
    "                targets[i]['boxes'] = targets[i]['boxes'].to(device=device)\n",
    "                targets[i]['labels'] = targets[i]['labels'].to(device=device)\n",
    "                targets[i]['image_id'] = targets[i]['image_id'].to(device=device)\n",
    "                targets[i]['area'] = targets[i]['area'].to(device=device)\n",
    "                targets[i]['iscrowd'] = targets[i]['iscrowd'].to(device=device)\n",
    "            imgs = imgs.to(device)\n",
    "#             print(imgs.device)\n",
    "#             print(targets[0]['boxes'].device)\n",
    "            images = list(image for image in imgs)\n",
    "            targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "#             counter += 1\n",
    "#             if counter == 10:\n",
    "#                 break\n",
    "        print(f\"Finished epoch (Loss: {loss})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "\n",
    "    :param batch: an iterable of N sets from __getitem__()\n",
    "    :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "    \"\"\"\n",
    "\n",
    "    images = list()\n",
    "    targets=list()\n",
    "\n",
    "    for i, t in batch:\n",
    "        images.append(i)\n",
    "        targets.append(t)\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE PRETRAINED MODEL\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# replace the classifier with a new one, that has num_classes which is user-defined\n",
    "num_classes = 2  # 1 class (planet) + background\n",
    "\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eNsebHpMyOKf"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, nesterov=True, momentum=0.9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iaUBwDKeyOKg",
    "outputId": "009b9c1b-d68c-478b-8d51-f55476dd5852"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning epoch 1\n",
      "labels tensor([ 1,  1, -1])\n",
      "labels tensor([ 1,  1, -1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f006911b0771>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-bc6a211f4e71>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, dataset, collate_fn, epochs)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0mregression_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbox_coder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatched_gt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             loss_objectness, loss_rpn_box_reg = self.compute_loss(\n\u001b[0;32m--> 365\u001b[0;31m                 objectness, pred_bbox_deltas, labels, regression_targets)\n\u001b[0m\u001b[1;32m    366\u001b[0m             losses = {\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34m\"loss_objectness\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_objectness\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torchvision/models/detection/rpn.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, objectness, pred_bbox_deltas, labels, regression_targets)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \"\"\"\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0msampled_pos_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_neg_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfg_bg_sampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0msampled_pos_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_pos_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0msampled_neg_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_neg_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages/torchvision/models/detection/_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, matched_idxs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mpos_idx_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mneg_idx_per_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# create binary mask from indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, train_dataset, collate_fn, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtGO4FIJyOKh",
    "outputId": "89f4f1b0-73f2-41ab-bade-4c9e517e3ab8"
   },
   "outputs": [],
   "source": [
    "check_accuracy(test_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKC1FEijyOKi",
    "outputId": "8a17a636-2886-4ac6-e69b-c0a048cf05a8"
   },
   "outputs": [],
   "source": [
    "check_accuracy(train_dataset, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "train.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "d9f5d24a26cbeabac4edb9e730c7a997b018b03d50c20600b25ccabbc453b115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
